{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h2><left>Hello, This Notebook Contains example of Email-Classification Algorithm </left></h2>\n",
    "<h4><left>- We classify emails as Spam or Ham based on scikit-learn Library</left></h4>\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">\n",
    "        <ul>\n",
    "            <li><h4>Contents:</h4></li>\n",
    "    <ul>\n",
    "        <li><a href=\"#load\">Loading Data</a></li>\n",
    "        <li><a href=\"#prepare\">Data Preparation</a></li>\n",
    "        <li><a href=\"#modeling\">Modeling</a></li>\n",
    "        <ul>\n",
    "            <li><a href=\"#log_reg\">Logistic Regression</a></li>\n",
    "            <li><a href=\"#another_models\">More Models</a></li>\n",
    "        </ul>\n",
    "    </ul>\n",
    "    </ul>\n",
    "</div>\n",
    "<hr>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"load\"></a>\n",
    "<h3><left>- Getting Data:</left></h3>\n",
    "<h4><left>- Loading, Reading and Extracting Texts. </left></h4>\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urllib\n",
    "import tarfile\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DOWNLOAD_ROOT= \"http://spamassassin.apache.org/old/publiccorpus/\"\n",
    "DATASETS_PATH= os.path.join(os.getcwd(),'datasets')\n",
    "\n",
    "pathes = {\n",
    "    'SpamDataSet':os.path.join(DATASETS_PATH,'spam'),\n",
    "}\n",
    "\n",
    "file_names ={\n",
    "    'Spam': \"20030228_spam.tar.bz2\",\n",
    "    'Ham':  \"20030228_easy_ham.tar.bz2\"\n",
    "}\n",
    "\n",
    "urls = {\n",
    "    'Spam': DOWNLOAD_ROOT + \"20030228_spam.tar.bz2\",\n",
    "    'Ham': DOWNLOAD_ROOT + \"20030228_easy_ham.tar.bz2\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(urls,file_names,direc):\n",
    "    print('start loading>>>')\n",
    "    if not os.path.isdir(direc):\n",
    "        os.makedirs(direc)\n",
    "    for filename, url in zip(file_names,urls):\n",
    "        file_path = os.path.join(direc, file_names[filename])\n",
    "        if not os.path.isfile(file_path):\n",
    "            print('loading: ',urls[url])\n",
    "            urllib.request.urlretrieve(urls[url], file_path)\n",
    "        tar_bz2_file = tarfile.open(file_path)\n",
    "        print('extracting to: ', file_path)\n",
    "        tar_bz2_file.extractall(path=direc)\n",
    "        tar_bz2_file.close()\n",
    "    print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "load_data(urls,file_names,pathes['SpamDataSet'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import email\n",
    "import email.policy\n",
    "\n",
    "def get_files_in_dir(direc):\n",
    "    files = [file_name for file_name in sorted(os.listdir(direc)) if len(file_name)>20 ]\n",
    "    return files\n",
    "def load_email(direc, file_name):\n",
    "    with open(os.path.join(direc,file_name),'rb') as f:\n",
    "        return email.parser.BytesParser(policy= email.policy.default).parse(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "SPAMS_DATA_PATH = os.path.join(pathes['SpamDataSet'],'spam')\n",
    "HAMS_DATA_PATH = os.path.join(pathes['SpamDataSet'],'easy_ham')\n",
    "\n",
    "spam_files= get_files_in_dir(SPAMS_DATA_PATH)\n",
    "ham_files= get_files_in_dir(HAMS_DATA_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(500, 2500)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(spam_files),len(ham_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "ham_emails = [load_email(HAMS_DATA_PATH,file_name) for file_name in ham_files]\n",
    "spam_emails = [load_email(SPAMS_DATA_PATH,file_name) for file_name in spam_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "\n",
    "def get_email_structure(email):\n",
    "    if isinstance(email, str):\n",
    "        return email\n",
    "    payload = email.get_payload()\n",
    "    if isinstance(payload, list):\n",
    "        return \"multipart({})\".format(\", \".join([\n",
    "            get_email_structure(sub_email)\n",
    "            for sub_email in payload\n",
    "        ]))\n",
    "    else:\n",
    "        return email.get_content_type()\n",
    "    \n",
    "def structures_counter(emails):\n",
    "    structures = Counter()\n",
    "    for email in emails:\n",
    "        structure = get_email_structure(email)\n",
    "        structures[structure] += 1\n",
    "    return structures\n",
    "\n",
    "def get_emails_by_type(e_type,emails):\n",
    "    return [email for email in emails\n",
    "                    if get_email_structure(email) == e_type]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'text/plain': 2408,\n",
       "         'multipart(text/plain, application/pgp-signature)': 66,\n",
       "         'multipart(text/plain, text/html)': 8,\n",
       "         'multipart(text/plain, text/enriched)': 1,\n",
       "         'multipart(text/plain, application/ms-tnef, text/plain)': 1,\n",
       "         'multipart(text/plain)': 3,\n",
       "         'multipart(text/plain, application/octet-stream)': 2,\n",
       "         'multipart(text/plain, text/plain)': 4,\n",
       "         'multipart(multipart(text/plain, text/plain, text/plain), application/pgp-signature)': 1,\n",
       "         'multipart(text/plain, video/mng)': 1,\n",
       "         'multipart(text/plain, multipart(text/plain))': 1,\n",
       "         'multipart(text/plain, application/x-pkcs7-signature)': 1,\n",
       "         'multipart(text/plain, multipart(text/plain, text/plain), text/rfc822-headers)': 1,\n",
       "         'multipart(text/plain, multipart(text/plain, text/plain), multipart(multipart(text/plain, application/x-pkcs7-signature)))': 1,\n",
       "         'multipart(text/plain, application/x-java-applet)': 1})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structures_counter(ham_emails)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Counter({'text/html': 183,\n",
       "         'text/plain': 218,\n",
       "         'multipart(text/plain, application/octet-stream)': 1,\n",
       "         'multipart(text/html)': 20,\n",
       "         'multipart(text/plain, text/html)': 45,\n",
       "         'multipart(text/plain)': 19,\n",
       "         'multipart(text/html, text/plain)': 1,\n",
       "         'multipart(text/html, application/octet-stream)': 2,\n",
       "         'multipart(multipart(text/html))': 5,\n",
       "         'multipart(text/plain, image/jpeg)': 3,\n",
       "         'multipart(multipart(text/html), application/octet-stream, image/jpeg)': 1,\n",
       "         'multipart(multipart(text/plain, text/html), image/gif)': 1,\n",
       "         'multipart/alternative': 1})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "structures_counter(spam_emails)#.most_common(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"prepare\"></a>\n",
    "<h3><left>- Data Preparation:</left></h3>\n",
    "<h4><left>- Splitting, Cleaning and Tokenization. </left></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "X_data= np.array(ham_emails+spam_emails)\n",
    "y_data = np.array([0]*len(ham_emails) + [1]*len(spam_emails))\n",
    "X_train,X_test,y_train,y_test = train_test_split(X_data,y_data,test_size=0.2,random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from html import unescape\n",
    "\n",
    "def html_to_text(html):\n",
    "    text = re.sub(r'<head.*?>.*?</head>','',html)\n",
    "    text = re.sub('<a\\s.*?>', ' HYPERLINK ', text, flags=re.M | re.S | re.I)\n",
    "    text = re.sub('<.*?>', '', text, flags=re.M | re.S)\n",
    "    text = re.sub(r'(\\s*\\n)+', '\\n', text, flags=re.M | re.S)\n",
    "    return  unescape(text)\n",
    "\n",
    "def email_to_text(email):\n",
    "    for part in email.walk():\n",
    "        ctype = part.get_content_type()\n",
    "        if ctype not in (\"text/plain\", \"text/html\"):\n",
    "            continue\n",
    "        try:\n",
    "            content = part.get_content()\n",
    "        except:\n",
    "            content = part.get_payload()\n",
    "        if ctype == 'text/plain':\n",
    "            return str(content)\n",
    "        elif ctype == 'text/html':\n",
    "            return html_to_text(content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import urlextract\n",
    "from sklearn.base import BaseEstimator , TransformerMixin\n",
    "from stemming.porter2 import stem\n",
    "\n",
    "url_extractor = urlextract.URLExtract()\n",
    "\n",
    "class emailToWordCounterTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self,**settings):\n",
    "        self.lower_case = settings.get('lower_case',True) \n",
    "        self.strip_header= settings.get('strip_header',True) \n",
    "        self.remove_punctuation= settings.get('remove_punctuation',True)  \n",
    "        self.replace_urls = settings.get('replace_urls',True) \n",
    "        self.replace_numbers = settings.get('replace_numbers',True)  \n",
    "        self.stemming= settings.get('stemming',True)\n",
    "\n",
    "    def fit(self,X,y=None):\n",
    "        return self\n",
    "    def transform(self,X,y=None):\n",
    "        X_transformed = []\n",
    "        for e in X:\n",
    "            text = email_to_text(e) or \"\"\n",
    "            if self.lower_case: \n",
    "                text = text.lower()\n",
    "                \n",
    "            if self.replace_urls and url_extractor is not None:\n",
    "                urls = list(set(url_extractor.find_urls(text)))\n",
    "                urls.sort(key=lambda url: len(url), reverse=True)\n",
    "                for url in urls:\n",
    "                    text = text.replace(url, \" url \")\n",
    "                    \n",
    "            if self.replace_numbers:\n",
    "                text = re.sub('\\d+(?:\\.\\d*(?:[eE]+\\d*)?)?','number',text)  \n",
    "                \n",
    "            if self.remove_punctuation:\n",
    "                text = re.sub('\\W+',' ',text,flags=re.M)\n",
    "                \n",
    "            words_count = Counter(text.split())\n",
    "            if self.stemming and stem is not None: \n",
    "                stemmed_words_count = Counter()\n",
    "                for (word, count) in words_count.items():\n",
    "                    stemmed_words_count[stem(word)]+= count\n",
    "                X_transformed.append(stemmed_words_count)\n",
    "                \n",
    "        return np.array(X_transformed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([Counter({'chuck': 1, 'murcko': 1, 'wrote': 1, 'stuff': 1, 'yawn': 1, 'r': 1}),\n",
       "       Counter({'the': 11, 'of': 9, 'and': 8, 'all': 3, 'christian': 3, 'to': 3, 'by': 3, 'jefferson': 2, 'i': 2, 'have': 2, 'superstit': 2, 'one': 2, 'on': 2, 'been': 2, 'has': 2, 'half': 2, 'rogueri': 2, 'teach': 2, 'jesus': 2, 'some': 1, 'interest': 1, 'quot': 1, 'url': 1, 'thoma': 1, 'examin': 1, 'known': 1, 'word': 1, 'do': 1, 'not': 1, 'find': 1, 'in': 1, 'our': 1, 'particular': 1, 'redeem': 1, 'featur': 1, 'they': 1, 'are': 1, 'alik': 1, 'found': 1, 'fabl': 1, 'mytholog': 1, 'million': 1, 'innoc': 1, 'men': 1, 'women': 1, 'children': 1, 'sinc': 1, 'introduct': 1, 'burnt': 1, 'tortur': 1, 'fine': 1, 'imprison': 1, 'what': 1, 'effect': 1, 'this': 1, 'coercion': 1, 'make': 1, 'world': 1, 'fool': 1, 'other': 1, 'hypocrit': 1, 'support': 1, 'error': 1, 'over': 1, 'earth': 1, 'six': 1, 'histor': 1, 'american': 1, 'john': 1, 'e': 1, 'remsburg': 1, 'letter': 1, 'william': 1, 'short': 1, 'again': 1, 'becom': 1, 'most': 1, 'pervert': 1, 'system': 1, 'that': 1, 'ever': 1, 'shone': 1, 'man': 1, 'absurd': 1, 'untruth': 1, 'were': 1, 'perpetr': 1, 'upon': 1, 'a': 1, 'larg': 1, 'band': 1, 'dupe': 1, 'import': 1, 'led': 1, 'paul': 1, 'first': 1, 'great': 1, 'corrupt': 1}),\n",
       "       Counter({'url': 4, 's': 3, 'group': 3, 'to': 3, 'in': 2, 'forteana': 2, 'martin': 2, 'an': 2, 'and': 2, 'we': 2, 'is': 2, 'yahoo': 2, 'unsubscrib': 2, 'y': 1, 'adamson': 1, 'wrote': 1, 'for': 1, 'altern': 1, 'rather': 1, 'more': 1, 'factual': 1, 'base': 1, 'rundown': 1, 'on': 1, 'hamza': 1, 'career': 1, 'includ': 1, 'his': 1, 'belief': 1, 'that': 1, 'all': 1, 'non': 1, 'muslim': 1, 'yemen': 1, 'should': 1, 'be': 1, 'murder': 1, 'outright': 1, 'know': 1, 'how': 1, 'unbias': 1, 'memri': 1, 'don': 1, 't': 1, 'html': 1, 'rob': 1, 'sponsor': 1, 'number': 1, 'dvds': 1, 'free': 1, 'p': 1, 'join': 1, 'now': 1, 'from': 1, 'this': 1, 'send': 1, 'email': 1, 'egroup': 1, 'com': 1, 'your': 1, 'use': 1, 'of': 1, 'subject': 1})],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "e2w = emailToWordCounterTransformer()\n",
    "co= e2w.fit_transform(X_train[:3])\n",
    "# e2w.lower_case\n",
    "co"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "class wordCounterToVectorTransformer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self, **settings):\n",
    "        self.voc_size = settings.get('voc_size',1000)\n",
    "        \n",
    "    def fit(self,X,y= None):\n",
    "        total_count = Counter()\n",
    "        for word_count in X:\n",
    "            for (word,count) in word_count.items():\n",
    "                total_count[word]+= min(count,10)\n",
    "        self.most_common = total_count.most_common(self.voc_size)\n",
    "        self.vocabulary_ = {word: index+1 for index, (word,_) in enumerate(self.most_common)}\n",
    "        \n",
    "        return self\n",
    "    def transform(self,X,y=None):\n",
    "        rows = np.array([])\n",
    "        cols = np.array([])\n",
    "        data = np.array([])\n",
    "        for row, word_count in enumerate(X):\n",
    "            for word, count in word_count.items():\n",
    "                rows = np.append(rows,row)\n",
    "                cols = np.append(cols,self.vocabulary_.get(word,0))\n",
    "                data= np.append(data,count)\n",
    "        return csr_matrix((data, (rows, cols)),shape=(len(X), self.voc_size + 1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'the': 1,\n",
       " 'of': 2,\n",
       " 'and': 3,\n",
       " 'to': 4,\n",
       " 'url': 5,\n",
       " 'all': 6,\n",
       " 'in': 7,\n",
       " 'christian': 8,\n",
       " 'on': 9,\n",
       " 'by': 10}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w2v = wordCounterToVectorTransformer(voc_size=10)\n",
    "vec= w2v.fit_transform(co)\n",
    "\n",
    "vec.toarray()\n",
    "w2v.vocabulary_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"modeling\"></a>\n",
    "<h3><left>- Modeling:</left></h3>\n",
    "<h4><left>- Creating, Training and Validation Model Piplines. </left></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "features_preprocess = Pipeline([\n",
    "    ('e2c',emailToWordCounterTransformer()),\n",
    "    ('c2v',wordCounterToVectorTransformer())\n",
    "])\n",
    "X_train_transformed = features_preprocess.fit_transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"log_reg\"></a>\n",
    "<h4><I><left>- Logistic Regression Model. </left></I></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Using backend SequentialBackend with 1 concurrent workers.\n",
      "E:\\Applications\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:    0.1s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.985, total=   0.2s\n",
      "[CV]  ................................................................\n",
      "[CV] .................................... , score=0.985, total=   0.2s\n",
      "[CV]  ................................................................\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Applications\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:    0.3s remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV] .................................... , score=0.993, total=   0.2s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Applications\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n",
      "[Parallel(n_jobs=1)]: Done   3 out of   3 | elapsed:    0.5s finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.9874999999999999"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
    "score = cross_val_score(log_clf, X_train_transformed, y_train, cv=3, verbose=3)\n",
    "score.mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Precision: 96.88%\n",
      "Recall: 97.89%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "E:\\Applications\\Anaconda\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:764: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  extra_warning_msg=_LOGISTIC_SOLVER_CONVERGENCE_MSG)\n"
     ]
    }
   ],
   "source": [
    "# Test Logistic Regression:\n",
    "from sklearn.metrics import precision_score, recall_score\n",
    "\n",
    "X_test_transformed = features_preprocess.transform(X_test)\n",
    "\n",
    "log_clf = LogisticRegression(solver=\"lbfgs\", random_state=42)\n",
    "log_clf.fit(X_train_transformed, y_train)\n",
    "\n",
    "y_pred = log_clf.predict(X_test_transformed)\n",
    "\n",
    "print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_pred)))\n",
    "print(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_pred)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"another_models\"></a>\n",
    "<h4><left>- Another Models: SVC, NuSVC, SGD Classifier and more. </left></h4>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "def classification(model,X_train,X_test,y_train,y_test):\n",
    "    model.fit(X_train,y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    print(\"Accuracy: {:.2f}%\".format(100 * accuracy_score(y_test, y_pred)))\n",
    "    print(\"Precision: {:.2f}%\".format(100 * precision_score(y_test, y_pred)))\n",
    "    print(\"Recall: {:.2f}%\".format(100 * recall_score(y_test, y_pred)))\n",
    "    return { 'model' : model, 'predictions': y_pred }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC,NuSVC\n",
    "from sklearn.naive_bayes import MultinomialNB ,BernoulliNB\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "svc = SVC(gamma = 'auto')\n",
    "mnb = MultinomialNB()\n",
    "bnb =BernoulliNB()\n",
    "nu_svc = NuSVC()\n",
    "sgd = SGDClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 98.67%\n",
      "Precision: 96.77%\n",
      "Recall: 94.74%\n"
     ]
    }
   ],
   "source": [
    "mnb_ = classification(mnb,X_train_transformed,X_test_transformed,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 94.67%\n",
      "Precision: 81.19%\n",
      "Recall: 86.32%\n"
     ]
    }
   ],
   "source": [
    "bnb_ = classification(bnb,X_train_transformed,X_test_transformed,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.83%\n",
      "Precision: 86.54%\n",
      "Recall: 94.74%\n"
     ]
    }
   ],
   "source": [
    "sgd_ = classification(sgd,X_train_transformed,X_test_transformed,y_train,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 96.00%\n",
      "Precision: 100.00%\n",
      "Recall: 74.74%\n"
     ]
    }
   ],
   "source": [
    "svc_= classification(svc,X_train_transformed,X_test_transformed,y_train,y_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3><I><center>...The End... </center></I></h3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
